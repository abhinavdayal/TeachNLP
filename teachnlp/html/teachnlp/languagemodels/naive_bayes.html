<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>teachnlp.languagemodels.naive_bayes API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>teachnlp.languagemodels.naive_bayes</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np
from typing import Text, List, Any, Optional

class NaiveBayes:
  &#34;&#34;&#34;
  A class to implement Naive Bayes Language model for classification tasks.
  ...

  Attributes
  ----------
  vocabulary : set()
    Set of all words in training corpus
  class_priors : dict()
    prior probabilities of all classes. (key : value) &lt;==&gt; (class name : prior probability)
  classes_and_each_words_freq__dict : {class:{word1:freq,word2:freq,.....}}
    This dictionary contains all classes mapped with corresponding word-frequency dictionaries
  classes_word_count_dict : {class : Number of words in all documents}
    This dictionary contains each class mapped with number of words in all documents of that class.
  
  Methods
  -------
  fit(training_corpus: List[List[List[Text],Text]])
    fits the model with given training corpus
  predict_class(test_document : Text, smoothing: Text = None)
    Takes a document(may be sentence or a paragraph) and predicts the class for that document.
  &#34;&#34;&#34;

  def __init__(self):
    self.vocabulary = set() 
    self.class_priors_dict = dict()
    self.classes_and_each_words_freq__dict = dict()
    self.classes_word_count_dict = dict()


  def fit(self, training_corpus: List) -&gt; None:
    &#34;&#34;&#34;This function takes the trainig corpus and calculates the prior probabilities of each class 
    and counts the occurrence of every word in every possible class.
    Each training sample is document and corresponding class.
    ...

    Parameters
    ----------
    training_corpus : List[List[List[Text],Text]]
      training corpus is list of trainng examples. 
      Each example is [sentence,class]
      sentence is a list of words.
    &#34;&#34;&#34;
    class_count = {}
    for doc,cls in training_corpus: 
      self.classes_word_count_dict[cls] = self.classes_word_count_dict.get(
          cls, 0)+len(doc)
      class_count[cls] = class_count.get(cls,0)+1
      word_dict = self.classes_and_each_words_freq__dict.get(cls, {})
      for word in doc:
        self.vocabulary.add(word)
        word_dict[word] = word_dict.get(word,0)+1 
      self.classes_and_each_words_freq__dict[cls] = word_dict
    for cls in class_count:
      self.class_priors[cls] = np.log(class_count[cls]/len(training_corpus)) 
    print(&#34;Done!!!&#34;)

  def predict_class(self, test_document: Text, smoothing: Text = None) -&gt; Text:
    &#34;&#34;&#34;Predicts and returns the class of a document based on the prior probablities of class,likelihood of words.

    ...

    Parameters
    ----------
    test_document : Text
      testing sentence.
    smoothing : Text
      String representing the method to be followed for smoothing.
      refer page number 5-6 of https://web.stanford.edu/~jurafsky/slp3/4.pdf thi lesson to better understand smoothing
      smoothing should be laplace for laplace smoothing. 
    &#34;&#34;&#34;
    x = y = 0
    if(smoothing == &#34;laplace&#34;):
      x = 1
      y = len(self.vocabulary) 
    posteriors = {}
    for cls in self.class_priors:
      likelihood = 0 
      for word in test_document:
        likelihood += np.log((self.classes_and_each_words_freq__dict[cls].get(
            word, 0)+x)/(self.classes_word_count_dict[cls]+y))
      posteriors[cls] = likelihood+self.class_priors[cls] 
    return max(posteriors,key = lambda x:posteriors[x])</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="teachnlp.languagemodels.naive_bayes.NaiveBayes"><code class="flex name class">
<span>class <span class="ident">NaiveBayes</span></span>
</code></dt>
<dd>
<div class="desc"><p>A class to implement Naive Bayes Language model for classification tasks.
&hellip;</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>vocabulary</code></strong> :&ensp;<code>set()</code></dt>
<dd>&nbsp;</dd>
<dt>Set of all words in training corpus</dt>
<dt><strong><code>class_priors</code></strong> :&ensp;<code>dict()</code></dt>
<dd>&nbsp;</dd>
<dt>prior probabilities of all classes. (key : value) &lt;==&gt; (class name : prior probability)</dt>
<dt><strong><code>classes_and_each_words_freq__dict</code></strong> :&ensp;<code>{class:{word1:freq,word2:freq,.....}}</code></dt>
<dd>&nbsp;</dd>
<dt>This dictionary contains all classes mapped with corresponding word-frequency dictionaries</dt>
<dt><strong><code>classes_word_count_dict</code></strong> :&ensp;<code>{class : Number</code> of <code>words in all documents}</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>This dictionary contains each class mapped with number of words in all documents of that class.</p>
<h2 id="methods">Methods</h2>
<p>fit(training_corpus: List[List[List[Text],Text]])
fits the model with given training corpus
predict_class(test_document : Text, smoothing: Text = None)
Takes a document(may be sentence or a paragraph) and predicts the class for that document.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NaiveBayes:
  &#34;&#34;&#34;
  A class to implement Naive Bayes Language model for classification tasks.
  ...

  Attributes
  ----------
  vocabulary : set()
    Set of all words in training corpus
  class_priors : dict()
    prior probabilities of all classes. (key : value) &lt;==&gt; (class name : prior probability)
  classes_and_each_words_freq__dict : {class:{word1:freq,word2:freq,.....}}
    This dictionary contains all classes mapped with corresponding word-frequency dictionaries
  classes_word_count_dict : {class : Number of words in all documents}
    This dictionary contains each class mapped with number of words in all documents of that class.
  
  Methods
  -------
  fit(training_corpus: List[List[List[Text],Text]])
    fits the model with given training corpus
  predict_class(test_document : Text, smoothing: Text = None)
    Takes a document(may be sentence or a paragraph) and predicts the class for that document.
  &#34;&#34;&#34;

  def __init__(self):
    self.vocabulary = set() 
    self.class_priors_dict = dict()
    self.classes_and_each_words_freq__dict = dict()
    self.classes_word_count_dict = dict()


  def fit(self, training_corpus: List) -&gt; None:
    &#34;&#34;&#34;This function takes the trainig corpus and calculates the prior probabilities of each class 
    and counts the occurrence of every word in every possible class.
    Each training sample is document and corresponding class.
    ...

    Parameters
    ----------
    training_corpus : List[List[List[Text],Text]]
      training corpus is list of trainng examples. 
      Each example is [sentence,class]
      sentence is a list of words.
    &#34;&#34;&#34;
    class_count = {}
    for doc,cls in training_corpus: 
      self.classes_word_count_dict[cls] = self.classes_word_count_dict.get(
          cls, 0)+len(doc)
      class_count[cls] = class_count.get(cls,0)+1
      word_dict = self.classes_and_each_words_freq__dict.get(cls, {})
      for word in doc:
        self.vocabulary.add(word)
        word_dict[word] = word_dict.get(word,0)+1 
      self.classes_and_each_words_freq__dict[cls] = word_dict
    for cls in class_count:
      self.class_priors[cls] = np.log(class_count[cls]/len(training_corpus)) 
    print(&#34;Done!!!&#34;)

  def predict_class(self, test_document: Text, smoothing: Text = None) -&gt; Text:
    &#34;&#34;&#34;Predicts and returns the class of a document based on the prior probablities of class,likelihood of words.

    ...

    Parameters
    ----------
    test_document : Text
      testing sentence.
    smoothing : Text
      String representing the method to be followed for smoothing.
      refer page number 5-6 of https://web.stanford.edu/~jurafsky/slp3/4.pdf thi lesson to better understand smoothing
      smoothing should be laplace for laplace smoothing. 
    &#34;&#34;&#34;
    x = y = 0
    if(smoothing == &#34;laplace&#34;):
      x = 1
      y = len(self.vocabulary) 
    posteriors = {}
    for cls in self.class_priors:
      likelihood = 0 
      for word in test_document:
        likelihood += np.log((self.classes_and_each_words_freq__dict[cls].get(
            word, 0)+x)/(self.classes_word_count_dict[cls]+y))
      posteriors[cls] = likelihood+self.class_priors[cls] 
    return max(posteriors,key = lambda x:posteriors[x])</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="teachnlp.languagemodels.naive_bayes.NaiveBayes.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, training_corpus: List) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>This function takes the trainig corpus and calculates the prior probabilities of each class
and counts the occurrence of every word in every possible class.
Each training sample is document and corresponding class.
&hellip;</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>training_corpus</code></strong> :&ensp;<code>List[List[List[Text],Text]]</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>training corpus is list of trainng examples.
Each example is [sentence,class]
sentence is a list of words.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, training_corpus: List) -&gt; None:
  &#34;&#34;&#34;This function takes the trainig corpus and calculates the prior probabilities of each class 
  and counts the occurrence of every word in every possible class.
  Each training sample is document and corresponding class.
  ...

  Parameters
  ----------
  training_corpus : List[List[List[Text],Text]]
    training corpus is list of trainng examples. 
    Each example is [sentence,class]
    sentence is a list of words.
  &#34;&#34;&#34;
  class_count = {}
  for doc,cls in training_corpus: 
    self.classes_word_count_dict[cls] = self.classes_word_count_dict.get(
        cls, 0)+len(doc)
    class_count[cls] = class_count.get(cls,0)+1
    word_dict = self.classes_and_each_words_freq__dict.get(cls, {})
    for word in doc:
      self.vocabulary.add(word)
      word_dict[word] = word_dict.get(word,0)+1 
    self.classes_and_each_words_freq__dict[cls] = word_dict
  for cls in class_count:
    self.class_priors[cls] = np.log(class_count[cls]/len(training_corpus)) 
  print(&#34;Done!!!&#34;)</code></pre>
</details>
</dd>
<dt id="teachnlp.languagemodels.naive_bayes.NaiveBayes.predict_class"><code class="name flex">
<span>def <span class="ident">predict_class</span></span>(<span>self, test_document: str, smoothing: str = None) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Predicts and returns the class of a document based on the prior probablities of class,likelihood of words.</p>
<p>&hellip;</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>test_document</code></strong> :&ensp;<code>Text</code></dt>
<dd>&nbsp;</dd>
<dt>testing sentence.</dt>
<dt><strong><code>smoothing</code></strong> :&ensp;<code>Text</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>String representing the method to be followed for smoothing.
refer page number 5-6 of <a href="https://web.stanford.edu/~jurafsky/slp3/4.pdf">https://web.stanford.edu/~jurafsky/slp3/4.pdf</a> thi lesson to better understand smoothing
smoothing should be laplace for laplace smoothing.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_class(self, test_document: Text, smoothing: Text = None) -&gt; Text:
  &#34;&#34;&#34;Predicts and returns the class of a document based on the prior probablities of class,likelihood of words.

  ...

  Parameters
  ----------
  test_document : Text
    testing sentence.
  smoothing : Text
    String representing the method to be followed for smoothing.
    refer page number 5-6 of https://web.stanford.edu/~jurafsky/slp3/4.pdf thi lesson to better understand smoothing
    smoothing should be laplace for laplace smoothing. 
  &#34;&#34;&#34;
  x = y = 0
  if(smoothing == &#34;laplace&#34;):
    x = 1
    y = len(self.vocabulary) 
  posteriors = {}
  for cls in self.class_priors:
    likelihood = 0 
    for word in test_document:
      likelihood += np.log((self.classes_and_each_words_freq__dict[cls].get(
          word, 0)+x)/(self.classes_word_count_dict[cls]+y))
    posteriors[cls] = likelihood+self.class_priors[cls] 
  return max(posteriors,key = lambda x:posteriors[x])</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="teachnlp.languagemodels" href="index.html">teachnlp.languagemodels</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="teachnlp.languagemodels.naive_bayes.NaiveBayes" href="#teachnlp.languagemodels.naive_bayes.NaiveBayes">NaiveBayes</a></code></h4>
<ul class="">
<li><code><a title="teachnlp.languagemodels.naive_bayes.NaiveBayes.fit" href="#teachnlp.languagemodels.naive_bayes.NaiveBayes.fit">fit</a></code></li>
<li><code><a title="teachnlp.languagemodels.naive_bayes.NaiveBayes.predict_class" href="#teachnlp.languagemodels.naive_bayes.NaiveBayes.predict_class">predict_class</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>